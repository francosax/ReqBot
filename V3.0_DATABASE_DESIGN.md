# ReqBot v3.0 - Database Backend Design

**Feature**: Database Backend
**Priority**: High (Foundation for enterprise features)
**Effort**: 3-4 weeks
**Status**: In Progress
**Branch**: claude/v3.0-enterprise-features-01F6J6deMzMmYLCaJU3KcyfF

---

## ğŸ“‹ Overview

This document outlines the design and implementation of a database backend for ReqBot v3.0, replacing the current file-based storage with a robust database system that supports version history, advanced querying, and multi-user scenarios.

---

## ğŸ¯ Goals

### Primary Goals
1. **Persistent Storage**: Store all extracted requirements in a database
2. **Version History**: Track changes to requirements over time
3. **Advanced Querying**: Enable complex searches and filters
4. **Foundation for Enterprise**: Prepare for REST API and web-based version
5. **Backward Compatibility**: Continue supporting Excel/PDF export

### Non-Goals (Future Versions)
- Multi-user authentication (v3.0 User Management)
- Real-time collaboration (v3.0 Collaborative Workflow)
- Cloud database hosting (Future enhancement)

---

## ğŸ—„ï¸ Database Choice: SQLite + PostgreSQL Support

### Primary: SQLite
- **Pros**: Zero configuration, embedded, portable, perfect for desktop app
- **Cons**: Limited concurrency (fine for single-user desktop)
- **Use Case**: Default for desktop application

### Secondary: PostgreSQL (Future)
- **Pros**: Enterprise-grade, excellent concurrency, advanced features
- **Cons**: Requires setup and management
- **Use Case**: Web-based version, multi-user scenarios

### Strategy
- Use SQLAlchemy ORM for database abstraction
- Support both SQLite and PostgreSQL through same codebase
- Default to SQLite, allow PostgreSQL configuration

---

## ğŸ“Š Database Schema Design

### Core Tables

#### 1. `projects`
Represents a processing project (input folder + output folder)

```sql
CREATE TABLE projects (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    input_folder_path VARCHAR(500) NOT NULL,
    output_folder_path VARCHAR(500) NOT NULL,
    compliance_matrix_template VARCHAR(500),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_active BOOLEAN DEFAULT TRUE,
    metadata JSON  -- Store additional project settings
);
```

**Indexes**: `created_at`, `is_active`, `name`

---

#### 2. `documents`
Represents source PDF documents

```sql
CREATE TABLE documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,
    filename VARCHAR(255) NOT NULL,
    file_path VARCHAR(500) NOT NULL,
    file_hash VARCHAR(64) NOT NULL,  -- MD5/SHA256 for change detection
    file_size_bytes INTEGER,
    page_count INTEGER,
    processing_status VARCHAR(50) DEFAULT 'pending',  -- pending, processing, completed, failed
    processed_at TIMESTAMP,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    metadata JSON,  -- Store PDF metadata

    FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE,
    UNIQUE(project_id, file_hash)  -- Prevent duplicate processing
);
```

**Indexes**: `project_id`, `file_hash`, `processing_status`, `processed_at`

---

#### 3. `requirements`
Core table for extracted requirements

```sql
CREATE TABLE requirements (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER NOT NULL,
    project_id INTEGER NOT NULL,

    -- Requirement Identification
    label_number VARCHAR(100) NOT NULL,  -- e.g., "spec-Req#1-1"

    -- Requirement Content
    description TEXT NOT NULL,
    raw_text TEXT,  -- Original extracted text

    -- Location
    page_number INTEGER NOT NULL,

    -- Classification
    keyword VARCHAR(100),  -- Matching keyword (shall, must, etc.)
    priority VARCHAR(50),  -- high, medium, low, security
    category VARCHAR(100),  -- Functional, Safety, Performance, etc.

    -- Quality Metrics
    confidence_score FLOAT,  -- 0.0 to 1.0

    -- Processing Info
    extracted_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    extraction_method VARCHAR(100) DEFAULT 'spacy_nlp',  -- Future: 'llm', 'ml_classifier'

    -- Version Control
    version INTEGER DEFAULT 1,
    is_current BOOLEAN DEFAULT TRUE,
    parent_requirement_id INTEGER,  -- For tracking requirement evolution

    -- User Modifications
    is_manually_edited BOOLEAN DEFAULT FALSE,
    edited_at TIMESTAMP,

    -- Metadata
    metadata JSON,  -- Store additional fields (e.g., BASIL export info)

    FOREIGN KEY (document_id) REFERENCES documents(id) ON DELETE CASCADE,
    FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE,
    FOREIGN KEY (parent_requirement_id) REFERENCES requirements(id) ON DELETE SET NULL
);
```

**Indexes**:
- `document_id`
- `project_id`
- `is_current` (for querying active requirements)
- `confidence_score` (for quality filtering)
- `priority`
- `category`
- `extracted_at`

---

#### 4. `requirement_history`
Tracks all changes to requirements (version history)

```sql
CREATE TABLE requirement_history (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    requirement_id INTEGER NOT NULL,
    version INTEGER NOT NULL,

    -- Snapshot of requirement at this version
    description TEXT NOT NULL,
    priority VARCHAR(50),
    category VARCHAR(100),
    confidence_score FLOAT,

    -- Change tracking
    change_type VARCHAR(50),  -- created, updated, deleted, merged
    change_description TEXT,
    changed_by VARCHAR(100),  -- Future: user ID
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- Full snapshot
    snapshot_data JSON,  -- Complete requirement data at this point

    FOREIGN KEY (requirement_id) REFERENCES requirements(id) ON DELETE CASCADE
);
```

**Indexes**: `requirement_id`, `changed_at`, `version`

---

#### 5. `processing_sessions`
Tracks each processing run for auditing and reporting

```sql
CREATE TABLE processing_sessions (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    project_id INTEGER NOT NULL,

    -- Session Info
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    status VARCHAR(50) DEFAULT 'running',  -- running, completed, failed, cancelled

    -- Configuration
    keywords_used TEXT,  -- Comma-separated keywords
    keyword_profile VARCHAR(100),  -- e.g., "Aerospace"
    confidence_threshold FLOAT,

    -- Results
    documents_processed INTEGER DEFAULT 0,
    requirements_extracted INTEGER DEFAULT 0,

    -- Quality Metrics
    avg_confidence_score FLOAT,
    min_confidence_score FLOAT,
    max_confidence_score FLOAT,

    -- Outputs
    excel_output_path VARCHAR(500),
    basil_output_path VARCHAR(500),
    pdf_output_paths TEXT,  -- JSON array of paths
    report_output_path VARCHAR(500),

    -- Performance
    processing_time_seconds FLOAT,

    -- Issues
    warnings_count INTEGER DEFAULT 0,
    errors_count INTEGER DEFAULT 0,
    warnings TEXT,  -- JSON array of warning messages
    errors TEXT,  -- JSON array of error messages

    metadata JSON,

    FOREIGN KEY (project_id) REFERENCES projects(id) ON DELETE CASCADE
);
```

**Indexes**: `project_id`, `started_at`, `status`

---

#### 6. `keyword_profiles` (Optional - May use existing JSON file)
Store custom keyword profiles

```sql
CREATE TABLE keyword_profiles (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    name VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    keywords TEXT NOT NULL,  -- JSON array of keywords
    is_predefined BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

---

### Supporting Tables (Future Enhancements)

#### 7. `users` (v3.0 - User Management)
```sql
CREATE TABLE users (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    username VARCHAR(100) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    password_hash VARCHAR(255),
    role VARCHAR(50) DEFAULT 'user',  -- admin, user, viewer
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_login_at TIMESTAMP
);
```

#### 8. `requirement_links` (v3.0 - Traceability)
```sql
CREATE TABLE requirement_links (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    source_requirement_id INTEGER NOT NULL,
    target_requirement_id INTEGER NOT NULL,
    link_type VARCHAR(50),  -- depends_on, implements, tests, derives_from
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    FOREIGN KEY (source_requirement_id) REFERENCES requirements(id) ON DELETE CASCADE,
    FOREIGN KEY (target_requirement_id) REFERENCES requirements(id) ON DELETE CASCADE,
    UNIQUE(source_requirement_id, target_requirement_id, link_type)
);
```

---

## ğŸ—ï¸ Architecture

### Layer Structure

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Presentation Layer                â”‚
â”‚   - main_app.py (GUI)               â”‚
â”‚   - REST API (Future)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Service Layer                     â”‚
â”‚   - database_service.py             â”‚
â”‚   - requirement_service.py          â”‚
â”‚   - project_service.py              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Access Layer (ORM)           â”‚
â”‚   - models.py (SQLAlchemy models)   â”‚
â”‚   - database.py (DB connection)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Database Layer                    â”‚
â”‚   - SQLite (default)                â”‚
â”‚   - PostgreSQL (optional)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ File Structure

```
ReqBot/
â”œâ”€â”€ database/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ models.py              # SQLAlchemy models
â”‚   â”œâ”€â”€ database.py            # Database connection & session management
â”‚   â”œâ”€â”€ migrations/            # Alembic migration files
â”‚   â”‚   â”œâ”€â”€ env.py
â”‚   â”‚   â””â”€â”€ versions/
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ project_service.py      # Project CRUD operations
â”‚   â”‚   â”œâ”€â”€ document_service.py     # Document CRUD operations
â”‚   â”‚   â”œâ”€â”€ requirement_service.py  # Requirement CRUD operations
â”‚   â”‚   â””â”€â”€ session_service.py      # Processing session tracking
â”‚   â””â”€â”€ repositories/          # Optional: Repository pattern
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_repository.py
â”‚       â”œâ”€â”€ project_repository.py
â”‚       â””â”€â”€ requirement_repository.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ database_config.py     # Database configuration
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ test_database/
â”‚       â”œâ”€â”€ test_models.py
â”‚       â”œâ”€â”€ test_services.py
â”‚       â””â”€â”€ test_integration.py
â””â”€â”€ reqbot.db                  # SQLite database file (gitignored)
```

---

## ğŸ”§ Technology Stack

### Core Dependencies
- **SQLAlchemy 2.0+**: ORM and database toolkit
- **Alembic**: Database migrations
- **SQLite**: Default database (built-in Python)
- **psycopg2-binary**: PostgreSQL driver (optional)

### Additional Utilities
- **pydantic**: Data validation (optional, for service layer)
- **python-dateutil**: Advanced date handling

---

## ğŸš€ Implementation Plan

### Phase 1: Foundation (Week 1)
- [x] Design database schema
- [ ] Set up SQLAlchemy ORM
- [ ] Create models.py with all tables
- [ ] Implement database.py for connection management
- [ ] Set up Alembic for migrations
- [ ] Create initial migration

### Phase 2: Service Layer (Week 1-2)
- [ ] Implement project_service.py
- [ ] Implement document_service.py
- [ ] Implement requirement_service.py
- [ ] Implement session_service.py
- [ ] Add comprehensive error handling
- [ ] Add transaction management

### Phase 3: Integration (Week 2-3)
- [ ] Modify RB_coordinator.py to use database
- [ ] Update processing_worker.py for session tracking
- [ ] Integrate with existing Excel export
- [ ] Integrate with BASIL export
- [ ] Add database initialization on app startup
- [ ] Implement backward compatibility mode

### Phase 4: Testing & Documentation (Week 3-4)
- [ ] Write unit tests for models
- [ ] Write integration tests for services
- [ ] Write end-to-end tests
- [ ] Performance testing and optimization
- [ ] Add indexes for common queries
- [ ] Document database schema
- [ ] Create migration guide
- [ ] Update CLAUDE.md

---

## ğŸ”„ Data Flow

### New Processing Workflow (with Database)

```
1. User starts processing
   â†“
2. Create/Get Project record in DB
   â†“
3. Create Processing Session record
   â†“
4. FOR EACH PDF:
   â”œâ”€â†’ Create/Update Document record
   â”œâ”€â†’ Check file_hash to detect changes
   â”œâ”€â†’ Extract requirements (existing NLP logic)
   â”œâ”€â†’ Insert Requirements records to DB
   â”œâ”€â†’ Create Requirement History records
   â””â”€â†’ Update Document processing_status
   â†“
5. Generate outputs (Excel, BASIL, PDF)
   â”œâ”€â†’ Query requirements from DB
   â”œâ”€â†’ Generate files (existing logic)
   â””â”€â†’ Store output paths in Processing Session
   â†“
6. Update Processing Session (completed)
   â†“
7. Generate HTML report (from DB data)
```

---

## ğŸ” Key Features

### 1. Version History Tracking

Every requirement change is tracked:
- **Initial extraction**: Creates requirement v1
- **Re-processing same PDF**:
  - If changed: Creates v2, marks v1 as `is_current=false`
  - If unchanged: Skips (based on file_hash)
- **Manual edits**: Creates new version with `is_manually_edited=true`

### 2. Incremental Processing

```python
# Pseudo-code
def should_process_document(file_path, project_id):
    current_hash = calculate_file_hash(file_path)
    existing_doc = db.query(Document).filter(
        Document.project_id == project_id,
        Document.file_path == file_path
    ).first()

    if existing_doc is None:
        return True  # New document

    if existing_doc.file_hash != current_hash:
        return True  # Document changed

    return False  # Already processed, no changes
```

### 3. Advanced Querying

```python
# Find all high-priority requirements with low confidence
requirements = db.query(Requirement).filter(
    Requirement.priority == 'high',
    Requirement.confidence_score < 0.6,
    Requirement.is_current == True
).all()

# Get requirement history
history = db.query(RequirementHistory).filter(
    RequirementHistory.requirement_id == req_id
).order_by(RequirementHistory.version).all()

# Find duplicates across documents
duplicates = db.query(Requirement.description, func.count()).group_by(
    Requirement.description
).having(func.count() > 1).all()
```

### 4. Project Management

- Create projects for different specification sets
- Track processing history per project
- Compare results across processing sessions
- Rollback to previous versions

---

## ğŸ“Š Backward Compatibility

### Dual Mode Operation

**Mode 1: Database-First (v3.0)**
- All data stored in database
- Excel/PDF/BASIL generated from DB
- Support for querying, versioning, history

**Mode 2: Legacy/Export-Only (v2.x compatible)**
- Database optional
- Direct file-to-file processing (current behavior)
- Database used only for tracking (if enabled)

### Configuration

```python
# config/database_config.py
DATABASE_ENABLED = True  # Enable database storage
DATABASE_URL = "sqlite:///reqbot.db"  # Default SQLite
LEGACY_MODE = False  # True = skip database, direct export only
```

---

## ğŸ¨ User Experience Changes

### Minimal UI Changes Initially
- Processing works same as before
- Database operations happen transparently
- New optional features:
  - View processing history
  - Search across all projects
  - Compare requirement versions

### Future UI Enhancements (Post-Database)
- Project management panel
- Requirement browser/editor
- Version comparison tool
- Advanced search and filters

---

## ğŸ§ª Testing Strategy

### Unit Tests
- Test each model's CRUD operations
- Test service layer methods
- Test version history creation
- Test incremental processing logic

### Integration Tests
- Test full workflow with database
- Test database migrations
- Test SQLite and PostgreSQL compatibility
- Test concurrent access scenarios

### Performance Tests
- Benchmark insert performance (1000+ requirements)
- Benchmark query performance
- Test with large PDFs (100+ pages)
- Profile memory usage

---

## ğŸ“ˆ Performance Considerations

### Optimization Strategies
1. **Batch Inserts**: Use SQLAlchemy bulk operations
2. **Indexes**: Add indexes on frequently queried columns
3. **Connection Pooling**: Reuse database connections
4. **Lazy Loading**: Load relationships only when needed
5. **Caching**: Cache frequently accessed data

### Expected Performance
- **Insert Speed**: ~1000 requirements/second (SQLite)
- **Query Speed**: <100ms for typical searches
- **Database Size**: ~1KB per requirement (estimate: 10K reqs = 10MB)

---

## ğŸ” Security Considerations

### Data Protection
- No passwords stored initially (single-user desktop)
- File paths stored as-is (trusted local environment)
- Future: Encrypt sensitive fields for PostgreSQL deployment

### SQL Injection Prevention
- SQLAlchemy ORM prevents injection attacks
- Parameterized queries throughout
- Input validation in service layer

---

## ğŸš§ Migration Strategy

### For Existing Users

1. **First Run with Database**:
   - Detect no database exists
   - Create database schema
   - Import recent processing results (if available)
   - Continue with normal processing

2. **Opt-In**:
   - Database enabled by default but can be disabled
   - Legacy mode continues to work

3. **Data Import** (Future):
   - Tool to import historical Excel files
   - Reconstruct requirements from BASIL exports

---

## ğŸ“ Success Metrics

### Phase 1 Success Criteria
- [x] Database schema designed and documented
- [ ] All models created and tested
- [ ] Migrations working
- [ ] Basic CRUD operations functional

### Phase 2 Success Criteria
- [ ] Service layer complete
- [ ] Transaction management working
- [ ] Error handling comprehensive

### Phase 3 Success Criteria
- [ ] Integration with existing workflow
- [ ] No breaking changes to existing features
- [ ] Performance acceptable (no slowdown)

### Phase 4 Success Criteria
- [ ] Test coverage >80%
- [ ] Documentation complete
- [ ] Ready for v3.0 release

---

## ğŸ”œ Next Steps

1. âœ… Complete database design (this document)
2. ğŸ”„ Implement SQLAlchemy models
3. â³ Set up database connection management
4. â³ Create initial migration
5. â³ Implement service layer
6. â³ Integrate with existing workflow
7. â³ Write tests
8. â³ Update documentation

---

**Document Version**: 1.0
**Last Updated**: 2025-11-18
**Author**: Claude (Sonnet 4.5)
**Status**: Design Complete, Ready for Implementation

---

*This design provides a solid foundation for ReqBot's transformation into an enterprise-grade application while maintaining backward compatibility and minimizing disruption to existing users.*
